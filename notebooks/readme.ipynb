{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with IMAGESIM\n",
    "\n",
    "The following directions are intended to allow an engineer to set up the necessary infrastructure, build the required data assests, train an imagesim model, and deploy a similarity schema for a dedicated set of ARD assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "\n",
    "Before working with code, it will be necessary to set up some EC2 machines in the relevant places. This project has compute assests located in the _analytics-core-development (733530388139)_ account on Maxar AWS.\n",
    "\n",
    "### Buckets\n",
    "\n",
    "The main bucket used for all data assests, including ARD data, is located at _s3://imagesim-storage_. The following describe some prefixes and assests that currently exist there and which can be used for model development.\n",
    "\n",
    "* ard/: This is where all ARD order deliveries are stored and represents the raw image data used in this project\n",
    "* chips/: This is where all _chipped_ images are stored, and represent image inputs to machine learning models TODO filename explanation\n",
    "* code/: misc. python scripts. Non-relevant to developers\n",
    "* datasets/: This prefix stores references to \"dataset versioning.\" TODO\n",
    "* demo-ard/: Contains ARD image delivery and vectors that can be uilized for testing change detection\n",
    "* nodata-index.json: This file maps chips from the chips directory (v0.2) to valid imagery; ie, imagery that contains real data at evert pixel. \n",
    "\n",
    "### Machines\n",
    "\n",
    "There are two EC2 machines that need to be set up to run this project effciently, minimizing cost while optimizing compute. \n",
    "\n",
    "**datamaker**: The \"datamaker\" machine is used for acquiring, processing and analyzing data. It is intended to be used to create the data that will eventually be the inputs to the modeling part of the pipeline. This machine has the following configuration properties on AWS EC2:\n",
    "\n",
    "* Hardware: _m5.xlarge_\n",
    "* System: Ubuntu\n",
    "* AMI Id: ami-085925f297f89fce1\n",
    "* Storage: 500GB (EBS)\n",
    "* Username: ubuntu\n",
    "* Security group:\n",
    "\n",
    "The cost to run this machine is approx. $0.1 per compute hour.\n",
    "\n",
    "**trainer**: The \"trainer\" machine is used for training the model that is primarily for training, and is currently used to encode images from trained models. This is a GPU machineThis machine has the following configuration properties on AWS EC2:\n",
    "\n",
    "* Hardware: _p3.8xlarge_\n",
    "* System: Ubuntu\n",
    "* AMI Id: ami-019266bf7a55994a7\n",
    "* Storage: 1000GB (EBS)\n",
    "* Username: ubuntu\n",
    "* Security group:\n",
    "\n",
    "The cost to run this machine is approx. $12.00 per compute hour.\n",
    "\n",
    "\n",
    "#### Setting up cloud compute environments and filesystem \n",
    "\n",
    "Both machines specify AMIs that do not contain prescribed python compute environments, unlike other DL or data science AMIs. To work in the cloud with imagesim, it will be necessary to set up python environments via conda, which must be installed. The following directions show how to install conda on an Ubuntu system and how to install the relevant packages for imagesim.\n",
    "\n",
    "**Setting up the Datamaker Python environment**\n",
    "1. ssh into your datamaker instance: `ssh -i <path-to-your-private-sshkey> ubuntu@<datamaker's public IPv4 DNS address>`.\n",
    "2. Install [https://docs.conda.io/en/latest/miniconda.html](miniconda). This can be done by first downloading the installer to the system /tmp directory: `wget https://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh /tmp`. The installation can now be completed by running the installation script: `/bin/bash /tmp/Anaconda2-4.1.1-Linux-x86_64.sh`. The installation can be installed at the user's home directory, which is the default install location. To access the conda binary, either exit and reinitiate a new ssh tunnel, or simply run `source .bashrc`. You should now have miniconda installed and have access to the various conda commands for creating python environments, etc in your runtime path.\n",
    "3. Create a new python 3.8 environment called \"science\" and activate that environment: `conda create -n science python=3.8 -y && conda activate science`.\n",
    "4. Install the relevant packages, located in the imagesim repository. This can be done by installing the environment file located in the imagesim repo, or by manually installing the relevant packages: ...\n",
    "\n",
    "**Setting up the Datamaker filesystem**\n",
    "1. Create the _data_ directory: `mkdir /home/ubuntu/data`. This is the main subdirectory in which all actions and assets are located within the datamaker machine. The following instructions ought to be carried out with respect to this subdirectory.\n",
    "2. Create the notebooks directory: `mkdir /home/ubuntu/data/notebooks`.\n",
    "3. Create the chips directory: `mkdir /home/ubuntu/data/chips`.\n",
    "4. Create the ard directory: `mkdir /home/ubuntu/data/ard/`\n",
    "5. Download the relevant ARD data. The current project focuses on a subset of the ordered ARD data, namely those tiles in **UTM 33** that exist at _s3://imagesim-storage/ard/33_. Create the subdirectory for this data, `mkdir /home/ubuntu/data/ard/33`. Copy the This can be done by executing `aws s3 cp s3://imagesim-storage/ard/33 /home/ubuntu/data/ard/33 --recursive`.\n",
    "\n",
    "\n",
    "**Setting up the Trainer Python environment**\n",
    "1. Follow directions 1-4 as stated in the previous setup instructions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Datamaker\n",
    "\n",
    "Datamaker is the EC2 instance that is configured for creating OSM labels from ARD data, filtering and processing these data, and computing relevant statistical analysis on these data. It can also be used to run a TMS server to serve mosaiced ARD tiles (see section _Running a TMS on Mosaiced ARD data_). The following sections describe how imagesim modules and functions can be used to acquire OSM data, explore and filter that data, and make stasticical inferences relevant to subsequent model training.\n",
    "\n",
    "\n",
    "### OSM data acquisition\n",
    "\n",
    "After creating and setting up the datamaker instance as described in the previous sections, we can start the data engineering process by acquiring the relevant OSM data. The following code cell shows how the imagesim libary can be used to generate OSM data for the ARD tiles in question.\n",
    "\n",
    "```\n",
    "from imagesim.scripts.local.osm import fetch_osm_by_quadkey\n",
    "from imagesim.scripts.constants import DATA_PATH\n",
    "\n",
    "node_tags, way_tags, relation_tags = fetch_osm_by_quadkey(33, DATA_PATH)\n",
    "```\n",
    "\n",
    "This function takes a quadkey zone and a data path and writes out raw OSM queries (meaning no tag filtering) to `DATA_PATH`, which specifies your local ARD data path (ie, `/home/ubuntu/ard`). It looks up the various level 12 quadkeys that exist under the utm path in the ard structure, and writes out the results to that location to a file called _osm_data.json_.\n",
    "\n",
    "**osm_data.json**\n",
    "This data file structure is unique to the imagesim data pipeline. The file contains one dictionary object with the following keys:\n",
    "* quadkey: The level-12 quadkey which specifies the query geometry from which osm results were returned\n",
    "* nodes: The OSM nodes data parsed out from the raw Overpass results\n",
    "* ways: the OSM ways data parsed out from the raw Overpass results\n",
    "* relations: the OSM relations data parsed out from the raw Overpass results\n",
    "\n",
    "This filestructure reflects the importance of potentially treating different OSM elements individually in downstream applications.\n",
    "\n",
    "Once this function has completed, you ought to have an _osm_data.json_ at each ard quadkey subdirectory, ie the filepath `/home/ubuntu/data/ard/33/<level-12-quadkey>/osm_data.json` should exist for each quadkey.\n",
    "\n",
    "\n",
    "### ARD Image chipping and filtering \n",
    "\n",
    "ARD image creation can potentially deliver imagery that contains no-data values. Imagesim contains modules and functions for filtering and chipping ARD level-12 data tiles into smaller tiles that are used for model training. The following section describes how to create training-level chips and filter those results to include only true-data imagery.\n",
    "\n",
    "From the command line, invoke ARD chipping as follows:\n",
    "\n",
    "`python chip.py --ard-path /home/ubuntu/data/ard/33 --zoom 17 --proc 4 --dest /home/ununtu/data/chips/33`\n",
    "\n",
    "This will chip all the available ard imagery to the output chip path, at the zoom level specified. Supports multiprocessing, which is recommended.\n",
    "\n",
    "**Image chips: .../chips/33\\<imgchip\\>.jpg**\n",
    "Image chips, intended to for model consumption, have the filename structure `Z<utm-zone>-<quadkey>_<catalogid>.jpg`.\n",
    "The chip filename is prefixed by the letter Z and the UTM zone of origin, a hyphen delim, then the chip quadkey identifier at the _chip zoom level_, an underscore and finally the Maxar catalog id strip the image was derived from.\n",
    "\n",
    "It is worth noting that these chip images do not encode geospatial references and are simple jpgs. Their filename construct, however, contains the relevant information to map the chip to its geospatial coordinates. Extracting the catalog id from the filename can be used to map back to the origination ARD tile as well. Imagesim has a utility to do just this:\n",
    "\n",
    "`from imagesi.scripts.chip import get_cog_path_from_chip ...`\n",
    "\n",
    "This is very useful for downstream model QA tools, eg, cross-referencing model outputs to visualization services like TMS which consume the original ARD filesystem structure.\n",
    "\n",
    "\n",
    "[**Optional: Create nodata-index**]\n",
    "\n",
    "Sometimes the ARD data is delivered with no-data values in the event that you messed up the ordering api options I suppose. The following command will consume all of the generated chip imagery from the previous step, perform a \"no data\" test on the corpus, and write out a json-based result index mapping to each chip filename a classification of \"no data,\" \"partial data,\" or \"all data.\" It does not create, modify or delete any of the existing image files.\n",
    "\n",
    "`python chip.py --filename /home/ubuntu/data/nodata-index.json --chip-dir /home/ubuntu/data/chips/33`\n",
    "\n",
    "\n",
    "At this point, all of the raw data acquisition requirements are satisfied and in their proper place on the filesystem. The following section is intended as a guide by which the imagesim library and associated tools can be used for the purpose of data exploration, summarization, filtering and processing. \n",
    "\n",
    "\n",
    "## Exploring and operating on massive datasets/ Selecting and Constructing Training data \n",
    "\n",
    "Probably the most important part of the Imagesim pipeline, in terms of obtaining useful model ouputs, is the image/labeling construction. The raw data space is colossal in dimensionality, noisy, and unbalanced both in terms of sample sizes as well as spatial distribution, it can be assumed.\n",
    "\n",
    "To complicate matters further, it is also not necessarily obvious how the feature characterization might positively or negatively affect the underlying target distribution for either feature similarity embeddings as well as the classifcation space. \n",
    "\n",
    "Furthermore, multilabel classifcation problems are only just becoming an area of interest in the academic and research domain. Some relevant questions include: what subset of osm labels well characterize the feature space for a particular physical dimensionality space? Ie, how do things change if the images are chipped at zoom level 18 instead of 17? I will elaborate more on these questions later.\n",
    "\n",
    "Whatever approach is taken in sample definition, it should take into account some of the basic considerations that are standard for the case of multiclass classification on imagery, and go from there.\n",
    "\n",
    "Practically, the techincal requirements to implement any such strategies are very much non-trivial considering the massive amount of data that lives on the filesystem. Attempting to work with all of the data in memory is **not viable** for many computing opertions on common tabular data structures. \n",
    "\n",
    "\n",
    "<TODO: Pre-defined filtering assets, PYGEOS-based SRTree spatial joins (geopandas latest)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data structure: Dense vs Sparse Encodings\n",
    "\n",
    "The final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definintion for multilabel classification\n",
    "\n",
    "Our model definition is relatively straightforward; as a backbone, we use a pre-trained (ImageNet) Resnet50 with initially frozen weights. We want to attach a fully-convolutional feature classifier at the head, and will select one of these last layers for encoding a low dimensional representation of our feature space. We can expect that these last layers will learn a spatial embedding as a function of the physical image features that are well prepresented by our labeling schema... if we set it up right!\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io as skio\n",
    "\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(chip_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((chip_size, chip_size)),\n",
    "#    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "dataset = SkywayDataset(\"/home/ubuntu/data/sample_sparse_encodings.csv\",\n",
    "                        \"/home/ubuntu/data/chips\",\n",
    "                        transforms=train_transform)\n",
    "\n",
    "valid_no = int(len(dataset) * 0.20)\n",
    "\n",
    "training_set, validation_set = random_split(dataset, [len(dataset) - valid_no, valid_no])\n",
    "#print(f'''training set length: {len(training_set)}, validation set length: {len(validation_set)}''')\n",
    "\n",
    "dataloader = {\"train\": DataLoader(training_set, shuffle=True, batch_size=batch_size),\n",
    "              \"val\": DataLoader(validation_set, shuffle=True, batch_size=batch_size)}\n",
    "\n",
    "\n",
    "def visualize_label_dist(df):\n",
    "    fig1, ax1 = plt.subplots(figsize=(10,10))\n",
    "    df.iloc[:,1:].sum(axis=0).plot.pie(autopct='%1.1f%%', shadow=True, startangle=90, ax=ax1)\n",
    "    ax1.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_label_corr(df):\n",
    "    sns.heatmap(df.iloc[:,1:].corr(), cmap=\"RdYlBu\", vmin=-1, vmax=1)\n",
    "\n",
    "\n",
    "def visualize_image(idx, classes=classes):\n",
    "    fd = d.iloc[idx]\n",
    "    image = fd.Feature\n",
    "    label = fd[1:].tolist()\n",
    "    print(image)\n",
    "\n",
    "    image = Image.open(\"/home/ubuntu/data/chips/\" + image)\n",
    "    #print(image.shape)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.imshow(image)\n",
    "    ax.grid(False)\n",
    "    classes = np.array(classes)[np.array(label, dtype=np.bool)]\n",
    "    for i, s in enumerate(classes):\n",
    "        ax.text(0, i*20, s, verticalalignment='top', color='white', fontsize=16, weight='bold')\n",
    "    plt.show()\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "class SkywayDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transforms=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.df.iloc[idx]\n",
    "        image = Image.open(os.path.join(self.img_dir, d.Feature)).convert(\"RGB\")\n",
    "        label = torch.tensor(d[1:].tolist(), dtype = torch.float32)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "def create_head(num_features, num_classes, dropout_prob=0.5, activation_func=nn.ReLU):\n",
    "    features_lst = [num_features, num_features//2, num_features//4]\n",
    "    layers = list()\n",
    "    for in_f, out_f in zip(features_lst[:-1], features_lst[1:]):\n",
    "        layers.append(nn.Linear(in_f, out_f))\n",
    "        layers.append(activation_func())\n",
    "        layers.append(nn.BatchNorm1d(out_f))\n",
    "        if dropout_prob != 0:\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        layers.append(nn.Linear(features_lst[-1], num_classes))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultilabelResnetFC(pl.LightningModule):\n",
    "    '''\n",
    "    Resnet50 backbone with fully connected FC head for multilabel classification\n",
    "    '''\n",
    "    \n",
    "    transforms = {\n",
    "        'train': train_transform,\n",
    "        'val': eval_transform,\n",
    "        'test': eval_transform,\n",
    "    }\n",
    "    \n",
    "    self.__init__(self, hparams=None):\n",
    "        super.__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.\n",
    "        # Define the head\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def loss(*args):\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': {'training/loss': loss},\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        accuracy = (y == y_hat).float().mean()\n",
    "        return {'val_loss': self.loss(y_hat, y), 'accuracy': accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        accuracy = torch.stack([x['accuracy'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\n",
    "            'validation/accuracy': accuracy,\n",
    "            'validation/loss': avg_loss,\n",
    "        }\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'test_loss': self.loss(y_hat, y)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        return {'avg_test_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.hparams.learning_rate, momentum=self.hparams.momentum)\n",
    "\n",
    "    def dataloader(self, phase):\n",
    "        coco_path = os.path.join(self.hparams.data_path, f'{phase}.json')\n",
    "        dataset = CocoClassificationDataset(coco_path, transform=self.transforms[phase])\n",
    "        shuffle = phase == 'train'\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size, shuffle=shuffle, num_workers=4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader('train')\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return self.dataloader('val')\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        return self.dataloader('test')\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parser):\n",
    "        parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "        parser.add_argument('--momentum', type=float, default=0.9)\n",
    "        parser.add_argument('--batch_size', type=int, default=16)\n",
    "        parser.add_argument('--data_path', type=str, required=True)\n",
    "        return parser\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "def freeze_pretrained(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "\n",
    "top_head = create_head(num_features, 13)\n",
    "model.fc = top_head\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "sgdr_partial = lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=0.005)\n",
    "\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        result = []\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Keep track of training, validation loss\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            for data, target in data_loader[phase]:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                print(data.shape)\n",
    "\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    # Feed input\n",
    "                    output = model(data)\n",
    "                    # Calculate loss\n",
    "                    loss = criterion(output, target)\n",
    "                    predictions = torch.sigmoid(output).data > 0.35\n",
    "                    predictions = predictions.to(torch.float32)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        # Backwards pass: compute gradient of the loss w.r.t. model params\n",
    "                        loss.backward()\n",
    "                        # Update model params\n",
    "                        optimizer.step()\n",
    "                        # Zero the grad to stop accumulation\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    running_loss += loss.item() * data.size(0)\n",
    "                    running_corrects += f1_score(target.to(\"cpu\").to(torch.int).numpy(),\n",
    "                                                predictions.to(\"cpu\").to(torch.int).numpy(),\n",
    "                                                average=\"samples\") * data.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "        epoch_acc = running_corrects / len(data_loader[phase].dataset)\n",
    "\n",
    "        result.append('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding performance via Alternative Learning Strategies and SOTA Feature Embedding \n",
    "\n",
    "There are a variety of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
